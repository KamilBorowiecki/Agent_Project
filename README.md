# Projekt z modelem Bielik

Projekt korzysta z lokalnego modelu jÄ™zykowego **Bielik 4.5B** uruchamianego przez Ollama.

---

## ğŸ“‹ Wymagania wstÄ™pne

Przed uruchomieniem projektu upewnij siÄ™, Å¼e masz zainstalowane:

* **Python 3.10+**
* **pip**

### Instalacja Ollama

Pobierz i zainstaluj Ollama ze strony:
ğŸ‘‰ [https://ollama.com/](https://ollama.com/)

SprawdÅº, czy dziaÅ‚a:

```bash
ollama --version
```

---

## ğŸ“š Instalacja zaleÅ¼noÅ›ci Pythona

Projekt zawiera plik `requirements.txt` z wymaganymi bibliotekami.

Zainstaluj je poleceniem:

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Konfiguracja klucza API

Aby aplikacja dziaÅ‚aÅ‚a poprawnie, wymagany jest klucz API.

### Uzyskanie klucza API
Klucz API moÅ¼esz wygenerowaÄ‡ pod tym adresem:  
[https://aistudio.google.com/app/api-keys](https://aistudio.google.com/app/api-keys)

Zaloguj siÄ™ na konto Google i utwÃ³rz nowy klucz API.

### Dodanie klucza do pliku `.env`
W gÅ‚Ã³wnym katalogu projektu utwÃ³rz plik `.env` (jeÅ›li jeszcze nie istnieje), a nastÄ™pnie dodaj do niego klucz w poniÅ¼szej formie:

```env
API_KEY="..."
```
---

## ğŸ¤– Pobranie i dodanie modelu Bielik

Projekt korzysta z modelu **Bielik 4.5B**, ktÃ³ry trzeba pobraÄ‡ ze strony **Hugging Face**:

ğŸ‘‰ [Bielik 4.5B na Hugging Face](https://huggingface.co/speakleash/Bielik-4.5B-v3.0-Instruct-GGUF)

### Pobierz model:

```bash
ollama pull hf.co/speakleash/Bielik-4.5B-v3.0-Instruct-GGUF:Q8_0
```

> âš ï¸ Model jest duÅ¼y (kilka GB), wiÄ™c pobieranie moÅ¼e chwilÄ™ potrwaÄ‡.

### Sprawdzenie modelu:

```bash
ollama list
```

### Dodanie modelu do projektu

W kodzie Python wystarczy podaÄ‡ Å›cieÅ¼kÄ™/nazwÄ™ modelu w inicjalizacji:

```python
from langchain_ollama import ChatOllama

bielik_model = ChatOllama(
    model="hf.co/speakleash/Bielik-4.5B-v3.0-Instruct-GGUF:Q8_0",
    temperature=0.1
)
```

---

## â–¶ï¸ Uruchomienie projektu

Po wykonaniu powyÅ¼szych krokÃ³w, uruchom projekt przez **Streamlit**:

```bash
streamlit run agent.py
```

Otworzy siÄ™ interfejs webowy, przez ktÃ³ry moÅ¼na korzystaÄ‡ z modelu Bielik.

---

## â±ï¸ Uwaga dotyczÄ…ca wydajnoÅ›ci

- **Tworzenie embeddingu z PDF-a**: proces ten moÅ¼e zajÄ…Ä‡ **okoÅ‚o 12 minut**, w zaleÅ¼noÅ›ci od rozmiaru dokumentu.  
  Dopiero po zakoÅ„czeniu tego kroku moÅ¼liwe jest zadawanie pytaÅ„ do dokumentu.

- **Odpowiedzi modelu**: generowanie odpowiedzi przez model moÅ¼e potrwaÄ‡ **do 14 minut**, szczegÃ³lnie w przypadku bardziej zÅ‚oÅ¼onych pytaÅ„ lub duÅ¼ych dokumentÃ³w.

Prosimy o cierpliwoÅ›Ä‡ â€“ procesy te wymagajÄ… sporej iloÅ›ci zasobÃ³w i czasu.

---

## âœ… Podsumowanie krokÃ³w

1. Zainstaluj Python i pip
2. Zainstaluj Ollama
3. Zainstaluj zaleÅ¼noÅ›ci (`pip install -r requirements.txt`)
4. Pobierz model Bielik z Hugging Face i zaÅ‚aduj go w Ollama
5. Uruchom projekt: `streamlit run agent.py`

Po tym projekt powinien dziaÅ‚aÄ‡ poprawnie z mod


## ğŸ–¼ï¸ PrzykÅ‚ad dziaÅ‚ania

![Interfejs Streamlit z modelem Bielik](image.png)
![Interfejs Streamlit z modelem Bielik](image2.png)
